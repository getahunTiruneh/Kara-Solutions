{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telegram data scraping\n",
    "### Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from telethon import TelegramClient\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# Append the parent directory to sys.path\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# ignore warrnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Custom Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom scripts for image downloading and Telegram data scraping\n",
    "from scripts.photo_scraper import download_images\n",
    "from scripts.telegram_data_scraping import scrape_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio to handle event loop issues in Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch API Credentials from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch API credentials from the .env file using environment variables\n",
    "api_id = os.getenv('API_ID')\n",
    "api_hash = os.getenv('API_HASH')\n",
    "phone_number = os.getenv('PHONE_NUMBER')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to log messages to both a file and the console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"../logs/scraping.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Output logs to console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Telegram client using environment variables\n",
    "client = TelegramClient('session_name', api_id, api_hash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Scrape Messages from Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_channels(channel_urls):\n",
    "    \"\"\"\n",
    "    Scrapes images and messages from a list of Telegram channels.\n",
    "    Args:\n",
    "        channel_urls (list): List of Telegram channel URLs.\n",
    "    Returns:\n",
    "        final_df (DataFrame): Combined DataFrame containing messages from all channels.\n",
    "    \"\"\"\n",
    "    all_messages = []\n",
    "\n",
    "    for channel_url in channel_urls:\n",
    "        # Scrape and download images\n",
    "        # await scrape_images(channel_url)\n",
    "        \n",
    "        # Scrape messages and store in a DataFrame\n",
    "        df = await scrape_messages(channel_url, client)\n",
    "        all_messages.append(df)\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    final_df = pd.concat(all_messages, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Scrape Images from a Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape images from a channel\n",
    "async def scrape_images(channel_url, client):\n",
    "    \"\"\"\n",
    "    Scrape images from a Telegram channel and download them.\n",
    "\n",
    "    Args:\n",
    "        channel_url: The URL of the Telegram channel.\n",
    "        client: The Telegram client used to interact with the Telegram API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Scraping images from channel: {channel_url}\")\n",
    "        \n",
    "        # Get the channel entity\n",
    "        channel = await client.get_entity(channel_url)\n",
    "        channel_name = channel.username or channel.title.replace(' ', '_')\n",
    "\n",
    "        # Scrape messages and download images\n",
    "        async for message in client.iter_messages(channel):\n",
    "            await download_images(message, channel_name, client)\n",
    "        \n",
    "        logging.info(f\"Finished scraping images from channel: {channel_url}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping images from channel {channel_url}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Scraping Process in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Asynchronous function to run the scraping process for multiple Telegram channels.\n",
    "    \n",
    "    Scrapes messages from the specified channels, saves the data to a CSV file, and returns the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        df (DataFrame): DataFrame containing scraped messages from all specified Telegram channels.\n",
    "    \"\"\"\n",
    "    channel_urls = ['t.me/DoctorsET', 't.me/lobelia4cosmetics', 't.me/CheMed123', 't.me/EAHCI', 'yetenaweg']  \n",
    "    try:\n",
    "        # Start the Telegram client\n",
    "        await client.start(phone=phone_number)\n",
    "        \n",
    "        # Scrape channels and get the final DataFrame\n",
    "        df = await scrape_channels(channel_urls)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv('../data/scraped_telegram_messages.csv', index=False)\n",
    "        \n",
    "        logging.info(\"Data successfully saved to '../data/scraped_telegram_messages.csv'\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main process: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 23:00:29,014 - INFO - Scraping messages from channel: t.me/DoctorsET\n",
      "2024-10-11 23:00:44,350 - INFO - Finished scraping messages from channel: t.me/DoctorsET\n",
      "2024-10-11 23:00:44,353 - INFO - Scraping messages from channel: t.me/lobelia4cosmetics\n",
      "2024-10-11 23:01:10,577 - INFO - Finished scraping messages from channel: t.me/lobelia4cosmetics\n",
      "2024-10-11 23:01:10,579 - INFO - Scraping messages from channel: t.me/CheMed123\n",
      "2024-10-11 23:01:11,396 - INFO - Finished scraping messages from channel: t.me/CheMed123\n",
      "2024-10-11 23:01:11,397 - INFO - Scraping messages from channel: t.me/EAHCI\n",
      "2024-10-11 23:01:33,478 - INFO - Finished scraping messages from channel: t.me/EAHCI\n",
      "2024-10-11 23:01:33,480 - INFO - Scraping messages from channel: yetenaweg\n",
      "2024-10-11 23:01:45,443 - INFO - Finished scraping messages from channel: yetenaweg\n",
      "2024-10-11 23:01:46,558 - INFO - Data successfully saved to '../data/scraped_telegram_messages.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_idid</th>\n",
       "      <th>text</th>\n",
       "      <th>sender</th>\n",
       "      <th>channel</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DoctorsET_864</td>\n",
       "      <td>https://youtu.be/5DBoEm-8kmA?si=LDLuEecNfULJVD...</td>\n",
       "      <td>-1001102021238</td>\n",
       "      <td>DoctorsET</td>\n",
       "      <td>2023-12-18 17:04:02+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DoctorsET_863</td>\n",
       "      <td>·ã∂·ä≠·â∞·à≠·àµ ·ä¢·âµ·ãÆ·åµ·ã´ ·â† ·ä†·ã≤·àµ ·ä†·âÄ·à´·à®·â• ·â† ·â¥·àå·â™·ã•·äï ·çï·àÆ·åç·à´·àô·äï ·àà·àò·åÄ·àò·à≠ ·ä®...</td>\n",
       "      <td>-1001102021238</td>\n",
       "      <td>DoctorsET</td>\n",
       "      <td>2023-11-03 16:14:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DoctorsET_862</td>\n",
       "      <td>·àû·âµ ·â†·àµ·ä≥·à≠ \\n\\n·àà·àç·åÜ·âª·âΩ·äï ·ã®·àù·äì·à≤·ãò·ãç ·àù·à≥·âÉ ·à≥·äì·âÄ·ãç ·ä•·ãµ·àö·ã´·â∏·ãç·äï ·ã≠·âÄ·äï...</td>\n",
       "      <td>-1001102021238</td>\n",
       "      <td>DoctorsET</td>\n",
       "      <td>2023-10-02 16:37:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DoctorsET_861</td>\n",
       "      <td>·ä® HIV ·ã®·â∞·çà·ãà·à∞ ·à∞·ãç ·ä†·åã·å•·àü·âΩ·àÅ ·ã´·âÉ·àç ? ·çà·ãç·àµ ·ä•·äì ·àÖ·ä≠·àù·äì ?\\n\\n·àô...</td>\n",
       "      <td>-1001102021238</td>\n",
       "      <td>DoctorsET</td>\n",
       "      <td>2023-09-16 07:54:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DoctorsET_860</td>\n",
       "      <td>·â†·âÖ·à≠·â• ·åä·ãú ·â†·àÉ·åà·à´·âΩ·äï ·àã·ã≠ ·ä•·ã®·â∞·àµ·â∞·ãã·àà ·ã´·àà ·ã®·â∞·àò·à≥·à≥·ã≠ ·çÜ·â≥ ( Homos...</td>\n",
       "      <td>-1001102021238</td>\n",
       "      <td>DoctorsET</td>\n",
       "      <td>2023-09-01 16:16:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355</th>\n",
       "      <td>yetenaweg_5</td>\n",
       "      <td>·ã≠·àÖ ·ä†·ã≤·à± ·ã®·äÆ·àÆ·äì ·â´·ã≠·à®·àµ ·â†·àõ·ã≠·ä≠·àÆ·àµ·äÆ·çï ·àµ·à≠ ·à≤·â≥·ã≠ ·ã´·àà·ãç ·àù·àµ·àç ·äê·ãç·ç¢ ·äÆ...</td>\n",
       "      <td>-1001447066276</td>\n",
       "      <td>yetenaweg</td>\n",
       "      <td>2020-02-17 20:58:59+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6356</th>\n",
       "      <td>yetenaweg_4</td>\n",
       "      <td>·ä†·ã≤·à± ·ã®·äÆ·àÆ·äì ·â´·ã≠·à®·àµ (·â†·ä†·ã≤·àµ ·ã®·à≥·ã≠·äï·àµ ·àµ·àô COVID-19) ·ç£\\n·ä®·ã®·âµ ...</td>\n",
       "      <td>-1001447066276</td>\n",
       "      <td>yetenaweg</td>\n",
       "      <td>2020-02-17 20:55:46+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6357</th>\n",
       "      <td>yetenaweg_3</td>\n",
       "      <td></td>\n",
       "      <td>-1001447066276</td>\n",
       "      <td>yetenaweg</td>\n",
       "      <td>2020-02-17 20:55:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6358</th>\n",
       "      <td>yetenaweg_2</td>\n",
       "      <td></td>\n",
       "      <td>-1001447066276</td>\n",
       "      <td>yetenaweg</td>\n",
       "      <td>2020-01-17 01:35:09+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6359</th>\n",
       "      <td>yetenaweg_1</td>\n",
       "      <td></td>\n",
       "      <td>-1001447066276</td>\n",
       "      <td>yetenaweg</td>\n",
       "      <td>2020-01-17 01:28:32+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6360 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_idid                                               text  \\\n",
       "0     DoctorsET_864  https://youtu.be/5DBoEm-8kmA?si=LDLuEecNfULJVD...   \n",
       "1     DoctorsET_863  ·ã∂·ä≠·â∞·à≠·àµ ·ä¢·âµ·ãÆ·åµ·ã´ ·â† ·ä†·ã≤·àµ ·ä†·âÄ·à´·à®·â• ·â† ·â¥·àå·â™·ã•·äï ·çï·àÆ·åç·à´·àô·äï ·àà·àò·åÄ·àò·à≠ ·ä®...   \n",
       "2     DoctorsET_862  ·àû·âµ ·â†·àµ·ä≥·à≠ \\n\\n·àà·àç·åÜ·âª·âΩ·äï ·ã®·àù·äì·à≤·ãò·ãç ·àù·à≥·âÉ ·à≥·äì·âÄ·ãç ·ä•·ãµ·àö·ã´·â∏·ãç·äï ·ã≠·âÄ·äï...   \n",
       "3     DoctorsET_861  ·ä® HIV ·ã®·â∞·çà·ãà·à∞ ·à∞·ãç ·ä†·åã·å•·àü·âΩ·àÅ ·ã´·âÉ·àç ? ·çà·ãç·àµ ·ä•·äì ·àÖ·ä≠·àù·äì ?\\n\\n·àô...   \n",
       "4     DoctorsET_860  ·â†·âÖ·à≠·â• ·åä·ãú ·â†·àÉ·åà·à´·âΩ·äï ·àã·ã≠ ·ä•·ã®·â∞·àµ·â∞·ãã·àà ·ã´·àà ·ã®·â∞·àò·à≥·à≥·ã≠ ·çÜ·â≥ ( Homos...   \n",
       "...             ...                                                ...   \n",
       "6355    yetenaweg_5  ·ã≠·àÖ ·ä†·ã≤·à± ·ã®·äÆ·àÆ·äì ·â´·ã≠·à®·àµ ·â†·àõ·ã≠·ä≠·àÆ·àµ·äÆ·çï ·àµ·à≠ ·à≤·â≥·ã≠ ·ã´·àà·ãç ·àù·àµ·àç ·äê·ãç·ç¢ ·äÆ...   \n",
       "6356    yetenaweg_4  ·ä†·ã≤·à± ·ã®·äÆ·àÆ·äì ·â´·ã≠·à®·àµ (·â†·ä†·ã≤·àµ ·ã®·à≥·ã≠·äï·àµ ·àµ·àô COVID-19) ·ç£\\n·ä®·ã®·âµ ...   \n",
       "6357    yetenaweg_3                                                      \n",
       "6358    yetenaweg_2                                                      \n",
       "6359    yetenaweg_1                                                      \n",
       "\n",
       "             sender    channel                      date  \n",
       "0    -1001102021238  DoctorsET 2023-12-18 17:04:02+00:00  \n",
       "1    -1001102021238  DoctorsET 2023-11-03 16:14:39+00:00  \n",
       "2    -1001102021238  DoctorsET 2023-10-02 16:37:39+00:00  \n",
       "3    -1001102021238  DoctorsET 2023-09-16 07:54:32+00:00  \n",
       "4    -1001102021238  DoctorsET 2023-09-01 16:16:15+00:00  \n",
       "...             ...        ...                       ...  \n",
       "6355 -1001447066276  yetenaweg 2020-02-17 20:58:59+00:00  \n",
       "6356 -1001447066276  yetenaweg 2020-02-17 20:55:46+00:00  \n",
       "6357 -1001447066276  yetenaweg 2020-02-17 20:55:05+00:00  \n",
       "6358 -1001447066276  yetenaweg 2020-01-17 01:35:09+00:00  \n",
       "6359 -1001447066276  yetenaweg 2020-01-17 01:28:32+00:00  \n",
       "\n",
       "[6360 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the main function\n",
    "await main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Images from telegram channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous main function to run the scraping process\n",
    "async def main():\n",
    "    channel_urls = ['t.me/lobelia4cosmetics', 't.me/CheMed123']\n",
    "    try:\n",
    "        # Start the Telegram client\n",
    "        await client.start(phone=phone_number)\n",
    "        \n",
    "        # Scrape channels\n",
    "        for channel_url in channel_urls:\n",
    "            await scrape_images(channel_url, client)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 19:38:17,076 - INFO - Connecting to 149.154.167.51:443/TcpFull...\n",
      "2024-10-14 19:38:19,648 - INFO - Connection to 149.154.167.51:443/TcpFull complete!\n",
      "2024-10-14 19:38:21,249 - INFO - Phone migrated to 4\n",
      "2024-10-14 19:38:21,515 - INFO - Reconnecting to new data center 4\n",
      "2024-10-14 19:38:21,874 - INFO - Disconnecting from 149.154.167.51:443/TcpFull...\n",
      "2024-10-14 19:38:21,879 - INFO - Disconnection from 149.154.167.51:443/TcpFull complete!\n",
      "2024-10-14 19:38:21,881 - INFO - Connecting to 149.154.167.92:443/TcpFull...\n",
      "2024-10-14 19:38:24,462 - INFO - Connection to 149.154.167.92:443/TcpFull complete!\n",
      "2024-10-14 19:41:18,148 - INFO - Scraping images from channel: t.me/CheMed123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed in successfully as ¬£Gech üë´; remember to not break the ToS or you will risk an account ban!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 19:41:19,247 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:22,072 - INFO - Downloaded image from message 97 in CheMed123\n",
      "2024-10-14 19:41:22,097 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:24,671 - INFO - Downloaded image from message 96 in CheMed123\n",
      "2024-10-14 19:41:24,676 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:26,121 - INFO - Downloaded image from message 95 in CheMed123\n",
      "2024-10-14 19:41:26,126 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:28,183 - INFO - Downloaded image from message 94 in CheMed123\n",
      "2024-10-14 19:41:28,187 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:30,116 - INFO - Downloaded image from message 93 in CheMed123\n",
      "2024-10-14 19:41:30,208 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:32,893 - INFO - Downloaded image from message 92 in CheMed123\n",
      "2024-10-14 19:41:32,898 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:34,389 - INFO - Downloaded image from message 91 in CheMed123\n",
      "2024-10-14 19:41:34,395 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:37,374 - INFO - Downloaded image from message 90 in CheMed123\n",
      "2024-10-14 19:41:37,376 - INFO - No valid image found in message 89.\n",
      "2024-10-14 19:41:37,380 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:40,149 - INFO - Downloaded image from message 88 in CheMed123\n",
      "2024-10-14 19:41:40,284 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:43,164 - INFO - Downloaded image from message 87 in CheMed123\n",
      "2024-10-14 19:41:43,171 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:47,331 - INFO - Downloaded image from message 86 in CheMed123\n",
      "2024-10-14 19:41:47,339 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:48,740 - INFO - Downloaded image from message 85 in CheMed123\n",
      "2024-10-14 19:41:48,744 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:49,938 - INFO - Downloaded image from message 82 in CheMed123\n",
      "2024-10-14 19:41:49,972 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:52,448 - INFO - Downloaded image from message 81 in CheMed123\n",
      "2024-10-14 19:41:52,452 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:54,101 - INFO - Downloaded image from message 80 in CheMed123\n",
      "2024-10-14 19:41:54,105 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:56,921 - INFO - Downloaded image from message 79 in CheMed123\n",
      "2024-10-14 19:41:56,924 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:41:58,948 - INFO - Downloaded image from message 78 in CheMed123\n",
      "2024-10-14 19:41:58,971 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:00,976 - INFO - Downloaded image from message 77 in CheMed123\n",
      "2024-10-14 19:42:00,980 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:03,066 - INFO - Downloaded image from message 76 in CheMed123\n",
      "2024-10-14 19:42:03,069 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:04,557 - INFO - Downloaded image from message 75 in CheMed123\n",
      "2024-10-14 19:42:04,562 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:06,166 - INFO - Downloaded image from message 74 in CheMed123\n",
      "2024-10-14 19:42:06,169 - INFO - No valid image found in message 73.\n",
      "2024-10-14 19:42:06,211 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:08,972 - INFO - Downloaded image from message 71 in CheMed123\n",
      "2024-10-14 19:42:08,975 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:17,334 - INFO - Downloaded image from message 70 in CheMed123\n",
      "2024-10-14 19:42:17,338 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:21,385 - INFO - Downloaded image from message 67 in CheMed123\n",
      "2024-10-14 19:42:21,390 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:25,184 - INFO - Downloaded image from message 64 in CheMed123\n",
      "2024-10-14 19:42:25,186 - INFO - No valid image found in message 62.\n",
      "2024-10-14 19:42:25,191 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:27,398 - INFO - Downloaded image from message 60 in CheMed123\n",
      "2024-10-14 19:42:27,443 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:29,766 - INFO - Downloaded image from message 59 in CheMed123\n",
      "2024-10-14 19:42:29,771 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:30,952 - INFO - Downloaded image from message 58 in CheMed123\n",
      "2024-10-14 19:42:30,955 - INFO - No valid image found in message 57.\n",
      "2024-10-14 19:42:30,959 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:33,849 - INFO - Downloaded image from message 56 in CheMed123\n",
      "2024-10-14 19:42:33,857 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:36,565 - INFO - Downloaded image from message 55 in CheMed123\n",
      "2024-10-14 19:42:36,586 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:39,054 - INFO - Downloaded image from message 53 in CheMed123\n",
      "2024-10-14 19:42:39,058 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:40,502 - INFO - Downloaded image from message 52 in CheMed123\n",
      "2024-10-14 19:42:40,505 - INFO - No valid image found in message 51.\n",
      "2024-10-14 19:42:40,511 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:42,103 - INFO - Downloaded image from message 50 in CheMed123\n",
      "2024-10-14 19:42:42,108 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:43,348 - INFO - Downloaded image from message 49 in CheMed123\n",
      "2024-10-14 19:42:43,384 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:46,754 - INFO - Downloaded image from message 48 in CheMed123\n",
      "2024-10-14 19:42:46,758 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:48,070 - INFO - Downloaded image from message 46 in CheMed123\n",
      "2024-10-14 19:42:48,073 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:50,293 - INFO - Downloaded image from message 45 in CheMed123\n",
      "2024-10-14 19:42:50,298 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:51,557 - INFO - Downloaded image from message 44 in CheMed123\n",
      "2024-10-14 19:42:51,587 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:53,484 - INFO - Downloaded image from message 43 in CheMed123\n",
      "2024-10-14 19:42:53,488 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:56,881 - INFO - Downloaded image from message 41 in CheMed123\n",
      "2024-10-14 19:42:56,885 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:58,059 - INFO - Downloaded image from message 40 in CheMed123\n",
      "2024-10-14 19:42:58,063 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:42:59,384 - INFO - Downloaded image from message 39 in CheMed123\n",
      "2024-10-14 19:42:59,413 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:00,637 - INFO - Downloaded image from message 38 in CheMed123\n",
      "2024-10-14 19:43:00,641 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:01,944 - INFO - Downloaded image from message 34 in CheMed123\n",
      "2024-10-14 19:43:01,947 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:03,809 - INFO - Downloaded image from message 33 in CheMed123\n",
      "2024-10-14 19:43:03,815 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:05,366 - INFO - Downloaded image from message 32 in CheMed123\n",
      "2024-10-14 19:43:05,372 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:07,581 - INFO - Downloaded image from message 31 in CheMed123\n",
      "2024-10-14 19:43:07,585 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:08,704 - INFO - Downloaded image from message 30 in CheMed123\n",
      "2024-10-14 19:43:08,709 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:10,601 - INFO - Downloaded image from message 29 in CheMed123\n",
      "2024-10-14 19:43:10,604 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:11,770 - INFO - Downloaded image from message 27 in CheMed123\n",
      "2024-10-14 19:43:11,812 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:14,564 - INFO - Downloaded image from message 26 in CheMed123\n",
      "2024-10-14 19:43:14,567 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:15,677 - INFO - Downloaded image from message 25 in CheMed123\n",
      "2024-10-14 19:43:15,681 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:16,829 - INFO - Downloaded image from message 23 in CheMed123\n",
      "2024-10-14 19:43:16,833 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:18,097 - INFO - Downloaded image from message 22 in CheMed123\n",
      "2024-10-14 19:43:18,121 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:19,502 - INFO - Downloaded image from message 21 in CheMed123\n",
      "2024-10-14 19:43:19,506 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:22,114 - INFO - Downloaded image from message 20 in CheMed123\n",
      "2024-10-14 19:43:22,118 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:25,622 - INFO - Downloaded image from message 19 in CheMed123\n",
      "2024-10-14 19:43:25,625 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:27,029 - INFO - Downloaded image from message 18 in CheMed123\n",
      "2024-10-14 19:43:27,054 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:28,626 - INFO - Downloaded image from message 17 in CheMed123\n",
      "2024-10-14 19:43:28,630 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:30,755 - INFO - Downloaded image from message 15 in CheMed123\n",
      "2024-10-14 19:43:30,760 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:33,046 - INFO - Downloaded image from message 14 in CheMed123\n",
      "2024-10-14 19:43:33,053 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:36,086 - INFO - Downloaded image from message 13 in CheMed123\n",
      "2024-10-14 19:43:36,144 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:38,198 - INFO - Downloaded image from message 11 in CheMed123\n",
      "2024-10-14 19:43:38,202 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:40,658 - INFO - Downloaded image from message 10 in CheMed123\n",
      "2024-10-14 19:43:40,663 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:41,566 - INFO - Downloaded image from message 9 in CheMed123\n",
      "2024-10-14 19:43:41,568 - INFO - No valid image found in message 7.\n",
      "2024-10-14 19:43:41,570 - INFO - No valid image found in message 6.\n",
      "2024-10-14 19:43:41,572 - INFO - No valid image found in message 4.\n",
      "2024-10-14 19:43:41,575 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:42,083 - INFO - Downloaded image from message 3 in CheMed123\n",
      "2024-10-14 19:43:42,087 - INFO - Starting direct file download in chunks of 131072 at 0, stride 131072\n",
      "2024-10-14 19:43:43,121 - INFO - Downloaded image from message 2 in CheMed123\n",
      "2024-10-14 19:43:43,124 - INFO - No valid image found in message 1.\n",
      "2024-10-14 19:43:43,126 - INFO - Finished scraping images from channel: t.me/CheMed123\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Scraping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
